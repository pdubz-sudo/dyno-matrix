{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "import shapefile\n",
    "from collections import Counter\n",
    "from shapely.geometry import shape, Point\n",
    "from rtree import index\n",
    "from matrix_factorization import *\n",
    "from model_evaluations import *\n",
    "import random\n",
    "from secrets import randbelow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install geopandas didn't work and neither did the conda forge version\n",
    "for some reason. I used this instead.\n",
    "https://geoffboeing.com/2014/09/using-geopandas-windows/.\n",
    "\n",
    "\n",
    "1. make a virtual environment with the whole anaconda: conda create --name geoproject anaconda.\n",
    "2. Install wheels as so without having to do the PATH part.\n",
    "\n",
    "3. Install shapafile with pip install pyshp. \n",
    "\n",
    "Geopandas is able to read all the gis files and you can change the coordinate reference system easily from it. http://matthewrocklin.com/blog/work/2017/09/21/accelerating-geopandas-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Foursquare data from Kaggle. Categorize coordinates into their respective NYC borough by using boundary polygons from https://geo.nyu.edu/catalog/nyu-2451-34563. Coordinates not in any of these polygons are in New Jersey and will be removed.\n",
    "\n",
    "The newly made dataframe will be saved as a pickle for fast data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the polygon neighborhood boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\OneDrive\\securitas-assignment\\MASTER\\data\\nyc-polygons\\neighborhoods-high-res-spatial\\ZillowNeighborhoods-NY.shp'\n",
    "\n",
    "# Use proper Coordinate Reference Systems coordinates Latitude/Longitude.\n",
    "# http://geopandas.org/projections.html\n",
    "boroughs = geopandas.read_file(file_path).to_crs({'init': 'epsg:4326'})\n",
    "raw_nyc = boroughs[boroughs['City']=='New York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nyc.plot('County', categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nyc.plot('Name', categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/23871409/optimal-way-of-aggregating-geographic-points-with-python-shapely/24114565#24114565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop staten island and bronx because we don't have enough FS data for them.\n",
    "# 2. Reset the index\n",
    "raw_nyc = raw_nyc[(raw_nyc['County']!='Richmond') & (raw_nyc['County']!='Bronx')]#.reset_index(drop=True, inplace=True)\n",
    "raw_nyc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "### Find duplicate neighborhoods\n",
    "[k for k, v in Counter(raw_nyc['Name']).items() if v ==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Brooklyn Heights' have duplicate except different counties. \n",
    "# It's in brooklyn so drop the new york county row.\n",
    "raw_nyc[raw_nyc['Name']=='Brooklyn Heights']#.iloc[1]['geometry']\n",
    "raw_nyc.drop(116, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Murray Hill' neighborhoods have the same name but are in different boroughs.\n",
    "# Change the Name of the Queens Murray Hill to 'Murray Hill Queens'\n",
    "raw_nyc[raw_nyc['Name']=='Murray Hill']\n",
    "raw_nyc.loc[59, 'Name'] = 'Murray Hill Queens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vinegar Hill has duplicate entries with one of the\n",
    "# entries in the wrong County; New York. I will delete\n",
    "# that entry because the neighborhood is in Brooklyn.\n",
    "\n",
    "raw_nyc[raw_nyc['Name']=='Vinegar Hill']\n",
    "raw_nyc.drop(84, inplace=True)\n",
    "\n",
    "# Reset the index.\n",
    "nyc_polygons = raw_nyc.reset_index(drop=True)\n",
    "\n",
    "# Make room for memory.\n",
    "del raw_nyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use nyc neighborhood polygons to assign foursquare location to it's neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will take a 1-2 minutes to load because of the data parser.\n",
    "file_path = r'C:\\Users\\DS\\OneDrive\\securitas-assignment\\MASTER\\data\\original_data\\dataset_TSMC2014_NYC.csv'\n",
    "\n",
    "raw_fs = pd.read_csv(file_path, parse_dates=['utcTimestamp'])\n",
    "# Make a coordinate tuple. This is the format that compatible with the shape file read by geopandas.\n",
    "raw_fs['coordinates'] = list(zip(raw_fs.loc[:,'longitude'], raw_fs.loc[:, 'latitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make coordinates into Shapely points so they can be placed in polygons.\n",
    "raw_fs['point'] = raw_fs['coordinates'].apply(lambda x: Point(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the nyc polygons, from beginning of notebook, into a list.\n",
    "poly_list = nyc_polygons['geometry'].tolist()\n",
    "\n",
    "# Create rtree index finding points to it's neighborhood polygon.\n",
    "idx = index.Index()\n",
    "for pos, poly in enumerate(poly_list):\n",
    "    idx.insert(pos, poly.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hood_ind(p):\n",
    "    '''Finds which polygon the point belongs to in the polygon list.\n",
    "    Arguments:\n",
    "    p -- Shapely point\n",
    "    \n",
    "    Return:\n",
    "    hood_indexes -- list, list of polygon indexes the point is in.\n",
    "    '''\n",
    "    hood_indexes = [i for i in idx.intersection(p.coords[0]) if p.within(poly_list[i])]\n",
    "    \n",
    "    return hood_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_ind = raw_fs['point'].apply(find_hood_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a ind to neighborhood key value lookup pair.\n",
    "# This dictionary will also be used below when placing the \n",
    "# label location data into neighborhoods.\n",
    "ind2hood = dict(zip(np.arange(len(poly_list)), nyc_polygons.loc[:, 'Name']))\n",
    "\n",
    "# Use ind2hood index to convert the indexes to neighborhood names.\n",
    "raw_fs['neighborhood'] = ['Other' if i == [] else ind2hood[i[0]] for i in hood_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indexes for points that don't belong to any neighborhood polyogons\n",
    "# delete them because they will not have any crime data.\n",
    "no_neighborhood = np.where(raw_fs['neighborhood']=='Other')[0]\n",
    "raw_fs.drop(raw_fs.index[no_neighborhood], inplace=True)\n",
    "raw_fs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn neighborhood column into one-hots.\n",
    "fs = pd.concat([raw_fs, pd.get_dummies(raw_fs['neighborhood'])], axis=1)\n",
    "\n",
    "del raw_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as pickle so you don't have to re-run all the code above again.\n",
    "file_path = r'C:\\Users\\DS\\OneDrive\\securitas-assignment\\Neighborhood Names GIS\\data\\cleaned_foursquare_df.pkl'\n",
    "pd.to_pickle(fs, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clean and Set up theft labels per neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\OneDrive\\securitas-assignment\\MASTER\\data\\original_data\\NYPD_Complaint_Data_Historic.csv'\n",
    "raw_crime_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are years that are wrong dates such as 1015 which will throw errors.\n",
    "# I 'coerce' the parsing and these wrong ones will turn into 'NaT'.\n",
    "raw_crime_data['Date'] = pd.to_datetime(raw_crime_data['CMPLNT_FR_DT'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Do the same with time and then use 'dt.time' to get just the time. There are 24:00:00 which\n",
    "# doesn't really exist so let's 'coerce' them to 'NaT' because I don't know if they are meant to be\n",
    "# on the next day or the current day of the date.\n",
    "raw_crime_data['Time'] = pd.to_datetime(raw_crime_data['CMPLNT_TO_TM'], \n",
    "                                        format='%H:%M:%S', errors='coerce').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'NaT''s from 'Time' and 'Date' columns\n",
    "drop_indexes_date = np.where(raw_crime_data['Date'].isnull())[0]\n",
    "drop_indexes_time = np.where(raw_crime_data['Time'].isnull())[0]\n",
    "\n",
    "\n",
    "drop_list = np.concatenate((drop_indexes_date, drop_indexes_time), axis=0)\n",
    "\n",
    "raw_crime_data.drop(drop_list, axis=0, inplace=True)\n",
    "\n",
    "del drop_list, drop_indexes_date, drop_indexes_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows from range 04-03-2012 and 02-06-2013 which is our FourSquare data range.\n",
    "raw_crime_data = raw_crime_data.set_index('Date').loc['20120403': '20130216'].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to start combining 'Date', and 'Time' to make a timestamp.\n",
    "raw_crime_data = raw_crime_data.reset_index()\n",
    "\n",
    "# Couldn't figure out how to combine the 'Date' and 'Time' datetime objects together\n",
    "# so I turned them back into string and combined them.\n",
    "# I had to access 'dt' first.\n",
    "raw_crime_data['Date'] = raw_crime_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# I couldn't do this the same way as above and had to use lambda instead to get it to work.\n",
    "raw_crime_data['Time'] = raw_crime_data['Time'].apply(lambda x: x.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_crime_data['utcTimestamp'] = pd.to_datetime(raw_crime_data['Date'] + ' ' + raw_crime_data['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grand Larceny (theft df). Theft had a low p-value in the research paper.\n",
    "# Therfore, we will keep things simple and just build models from theft labels.\n",
    "theft_df = raw_crime_data[raw_crime_data['OFNS_DESC']=='GRAND LARCENY']\n",
    "\n",
    "del raw_crime_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start using the longitude and latitude to place the crime in it's neighborhood.\n",
    "theft_df['coordinates'] = list(zip(theft_df.loc[:,'Longitude'], theft_df.loc[:, 'Latitude']))\n",
    "\n",
    "# Make coordinates into Shapely points so they can be placed in polygons.\n",
    "theft_df['point'] = theft_df['coordinates'].apply(lambda x: Point(x))\n",
    "\n",
    "theft_hood_ind = theft_df['point'].apply(find_hood_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ind2hood dictionary to convert the indexes to neighborhood names.\n",
    "\n",
    "theft_df['neighborhood'] = ['Other' if i == [] else ind2hood[i[0]] for i in theft_hood_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indexes for points that don't belong to any neighborhood polyogons\n",
    "# delete them because they will not have any foursquare data.\n",
    "outside_neighborhood = np.where(theft_df['neighborhood']=='Other')[0]\n",
    "theft_df.drop(theft_df.index[outside_neighborhood], inplace=True)\n",
    "theft_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the neighborhood column into one-hots.\n",
    "theft_group = theft_df.groupby([pd.Grouper(key='utcTimestamp', freq='3H'), 'neighborhood']).count()\n",
    "cleaned_theft_labels = theft_group.iloc[:,0].unstack(level=1).fillna(0)\n",
    "\n",
    "del theft_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle for faster loading\n",
    "file_path = r'C:\\Users\\DS\\OneDrive\\securitas-assignment\\Neighborhood Names GIS\\data\\cleaned_neighborhood_theft_labels.pkl'\n",
    "pd.to_pickle(cleaned_theft_labels, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning done.\n",
    "# Starting the dynamic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\OneDrive\\securitas-assignment\\MASTER\\data\\cleaned_foursquare_df.pkl'\n",
    "\n",
    "fs = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2547 3-hour time intervals that have at least regional one datapoint in this dataset. If there are no events in the time intervals when grouping, the new dataframe will skip over the no-event time interval. Since I have many dynamic features that could possibly have different missing data, I will keep doing a \"left\" merge to the base 2547 rows 3-hour time interval frame so I have one consistent dimension shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fs = fs.groupby(pd.Grouper(key='utcTimestamp', freq='3H'))['userId'].count().reset_index().rename(columns={'userId': 'count'})\n",
    "base_fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor Count: Number of unique users in region r at time interval t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visitor = fs.groupby([pd.Grouper(key='utcTimestamp', freq='3H'),\n",
    "                      'neighborhood'])['userId'].nunique().unstack(level=1).reset_index()\n",
    "visitor_count = pd.merge(base_fs, visitor, on='utcTimestamp', how='left').drop('count', axis=1)\n",
    "visitor_count = visitor_count.set_index('utcTimestamp')\n",
    "\n",
    "# Make space for memory\n",
    "del visitor\n",
    "\n",
    "print('Number of time intervals: ', visitor_count.shape[0])\n",
    "visitor_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Frequency: Number of check-ins in region r and its neighborhood at        time interval t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs = fs.groupby([pd.Grouper(key='utcTimestamp', freq='3H'),\n",
    "                  'neighborhood'])['userId'].count().unstack(level=1).reset_index()\n",
    "obs_freq = pd.merge(base_fs, obs, on='utcTimestamp', how='left').drop('count', axis=1).set_index('utcTimestamp')\n",
    "\n",
    "# Make space for memory\n",
    "del obs\n",
    "\n",
    "print('Number of time intervals: ', obs_freq.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor entropy: Diversity of visitors in a location with respect to their visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You have do merge the user ids total number of visits to the region and then divide the \n",
    "# 2 columns to get the probability that they will be there at that time.\n",
    "visits_r_t = fs.groupby(['neighborhood', pd.Grouper(key='utcTimestamp',\n",
    "                                             freq='3H'), 'userId']).count().iloc[:,0]\n",
    "\n",
    "visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of checkins per user\n",
    "total_user_checkins = fs.groupby(['userId']).count().iloc[:,0]\n",
    "total_user_checkins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join on user id and now you have userid number of checkins in that time interval and \n",
    "# a column that shows their total checkins.\n",
    "fs_visits_r_t = pd.merge(visits_r_t.reset_index(), total_user_checkins.reset_index(),\n",
    "                   on=['userId']).rename(columns={'venueId_x': 'reg_time_visits',\n",
    "                                                  'venueId_y': 'total_visits'})\n",
    "\n",
    "# Make room for memory\n",
    "del visits_r_t, total_user_checkins\n",
    "\n",
    "fs_visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability that user will be at region at time interval.\n",
    "fs_visits_r_t['probability'] = fs_visits_r_t['reg_time_visits']/fs_visits_r_t['total_visits']\n",
    "fs_visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_visits_r_t['used_for_visitor_entropy'] = [-x*np.log2(x) for x in fs_visits_r_t['probability']]\n",
    "fs_visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all the individual user entropys for region r and time interval t.\n",
    "incomplete_entropy_df = fs_visits_r_t.groupby(['neighborhood', \n",
    "                                    'utcTimestamp'])['used_for_visitor_entropy'].sum().unstack(level=0).reset_index()\n",
    "\n",
    "visitor_entropy = pd.merge(base_fs, incomplete_entropy_df, \n",
    "                           how='left', on='utcTimestamp').drop('count', axis=1).set_index('utcTimestamp')\n",
    "\n",
    "# Make space in memory.\n",
    "del incomplete_entropy_df\n",
    "\n",
    "print('Number of time intervals: ', visitor_entropy.shape[0])\n",
    "visitor_entropy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Region Popularity: Assess popularity of region r at time interval t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many total checkins at all time intervals \n",
    "all_checkins_at_time = fs.groupby(pd.Grouper(key='utcTimestamp', freq='3H'))['userId'].count()\n",
    "\n",
    "# Divide regions checkins at time interval by total checkins at that time interval.\n",
    "region_popularity = obs_freq.divide(all_checkins_at_time, axis=0)\n",
    "\n",
    "\n",
    "del all_checkins_at_time\n",
    "\n",
    "print('Number of time intervals: ', region_popularity.shape[0])\n",
    "region_popularity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor Ratio:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group, count, and regroup, order by descending and take top 4\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby 'userId' and 'venueId'. Count to see how many times a user checked in that venueId. This will make a series.\n",
    "user_venue_count = fs.groupby(['userId', 'venueId']).count().iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you have the count for venue checkins by each user you will re-group by 'userId' and apply a function \n",
    "# to each 'userId' grouping (sort descending and keep top 4) to get back the top 4 visited venues for that user.\n",
    "user_grouping = user_venue_count.groupby(level=0, group_keys=False)  # When calling apply, add group keys argument to prevent key appearing twice IT.\n",
    "user_top_venues = user_grouping.apply(lambda x: x.sort_values(ascending=False).head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the timestamp and borough of all the users top 4 checkins.\n",
    "top_4_full_df = pd.merge(user_top_venues.reset_index(), fs, how='left', on=['userId', 'venueId'])\n",
    "\n",
    "# Drop duplicates so I get the first visit of the users top 4 checkins.\n",
    "first_visit = top_4_full_df.drop_duplicates(['userId', 'venueId'])\n",
    "incomplete_visit_df = first_visit.groupby([pd.Grouper(key='utcTimestamp', freq='3H'), 'neighborhood']).count().iloc[:, 0].unstack(level=1)\n",
    "\n",
    "##################### SANITY CHECK\n",
    "# # All that is commented out, to make sure the time stamps are ascending since I will drop duplicates.\n",
    "# # It seems like the timestamps are in order but just in case they are not I will order them because I\n",
    "# # am going to remove the duplicates based on userId and venueId so it will only keep the first occurence\n",
    "# # which gives up the first checkin timestamp.\n",
    "# sort_df = top_4_full_df.set_index(['userId', 'venueId']).groupby(level=[0,1], group_keys=False).apply(lambda x: x.sort_values(by='utcTimestamp'))\n",
    "# first_visit = sort_df.reset_index().drop_duplicates(['userId', 'venueId'])\n",
    "\n",
    "# # Group by time stamp and neighborhood. Take a count and unpivot the column so that we have the \n",
    "# # have the first time occurences of the top 4 userid visits by venue. The rows containing all zeros are excluded so\n",
    "# # this needs to be joined back to the base df so we have a row for each time interval.\n",
    "# incomplete_visit_df = first_visit.groupby([pd.TimeGrouper(key='utcTimestamp', freq='3H'), 'borough']).count().iloc[:, 0].unstack(level=1)\n",
    "######################\n",
    "\n",
    "# Join\n",
    "new_user_venue_checkins = pd.merge(base_fs, incomplete_visit_df.reset_index(), how='left', on='utcTimestamp').drop('count', axis=1)\n",
    "\n",
    "# Calculate visitor ratio by dividing by observation frequency for that region at that time interval.\n",
    "visitor_ratio = new_user_venue_checkins.set_index('utcTimestamp')/obs_freq\n",
    "\n",
    "# Make space for memory.\n",
    "del user_venue_count, user_grouping, user_top_venues, top_4_full_df\n",
    "del first_visit,incomplete_visit_df, new_user_venue_checkins\n",
    "\n",
    "print('Number of time intervals: ', visitor_ratio.shape[0])\n",
    "visitor_ratio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All dynamic features done except for one of them. Concat them all together, fill NaN with 0, and do the matrix factorization to make dynamic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat features and fill NaN with 0.\n",
    "original_features_df = pd.concat([visitor_count, \n",
    "           obs_freq,\n",
    "           visitor_entropy,\n",
    "           visitor_ratio,\n",
    "           region_popularity], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop low activity hours (less than 20 obs freq and low activity neighborhoods)\n",
    "# number_neighborhoods = obs_freq.columns.shape[0]\n",
    "# neighborhood_names = obs_freq.columns\n",
    "# low_activity_neighborhoods = neighborhood_names[obs_freq.fillna(0).sum()<300]\n",
    "# low_activity_hour_indexes = obs_freq[obs_freq.fillna(0).sum(axis=1)<20].index\n",
    "# original_features_df.drop(low_activity_neighborhoods, axis=1, inplace=True)\n",
    "# original_features_df.drop(low_activity_hour_indexes, inplace=True)\n",
    "\n",
    "# Drop low activity hours (less than 20 obs freq and low activity neighborhoods)\n",
    "number_neighborhoods = obs_freq.columns.shape[0]\n",
    "neighborhood_names = obs_freq.columns\n",
    "low_activity_hour_indexes = obs_freq[obs_freq.fillna(0).sum(axis=1)<20].index\n",
    "original_features_df.drop(low_activity_hour_indexes, inplace=True)\n",
    "\n",
    "\n",
    "low_activity_neighborhoods = neighborhood_names[obs_freq.fillna(0).sum()<300]\n",
    "original_features_df.drop(low_activity_neighborhoods, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = original_features_df.columns\n",
    "index = original_features_df.index\n",
    "K = 250  # Shape of second dimension of matrices. (time interval, K) x (K, regions)\n",
    "dynamic_features, _ = dynamic_feature_estimation(original_features_df.fillna(0).values, K=K, alpha=0.001, beta=0.001, epochs=20, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the df\n",
    "full_dynamic_df = pd.DataFrame(dynamic_features, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Labels and find city to build model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\OneDrive\\securitas-assignment\\MASTER\\data\\cleaned_neighborhood_theft_labels.pkl'\n",
    "theft_df = pd.read_pickle(file_path)\n",
    "\n",
    "# The author said the crime labels are binary so I make them binary.\n",
    "theft_df[theft_df>1]=1\n",
    "\n",
    "## Sanity check\n",
    "# np.where(np.asarray(theft_df)>1)\n",
    "\n",
    "# Get the time indexes that both X and Y df's match on, 'inner', so\n",
    "# I can start deciding which city I will run the model and can then actually\n",
    "# build the model.\n",
    "timestamp_indexes_keep = pd.merge(full_dynamic_df, theft_df,\n",
    "                                  on='utcTimestamp').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dynamic_df = original_features_df.loc[timestamp_indexes_keep]\n",
    "dynamic_df = full_dynamic_df.loc[timestamp_indexes_keep]\n",
    "theft_df = theft_df.loc[timestamp_indexes_keep]\n",
    "\n",
    "del timestamp_indexes_keep, original_features_df, full_dynamic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent that each neighborhood is labeled.\n",
    "neighborhoods = theft_df.columns\n",
    "percent_labeled = [int((sum(theft_df.loc[:,hood])/theft_df.shape[0])*100) for hood in neighborhoods]\n",
    "\n",
    "for city, percent in list(zip(neighborhoods, percent_labeled)):\n",
    "    print('{}%\\t{}'.format(percent, city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighborhood_evaluation(neighborhood):\n",
    "    '''The neighborhoods features are used to evaluate theft predictions with \n",
    "    LR, Random Forest, and fully connected 1-layer Neural Network and to compare original \n",
    "    sparse features, sparse scaled, dynamic features, and dynamic scaled. The evaluation \n",
    "    metrics are auc, f1, and accuracy. This function can be easily changed to return \n",
    "    predictions, auc, f1, and accuracy for all the differentcombinations of sparse, dynamic, \n",
    "    and scaled training data.\n",
    "    \n",
    "    Arguments:\n",
    "    neighborhood -- str, neighborhood name located in columns dataframe.\n",
    "    \n",
    "    Return:\n",
    "    None\n",
    "    '''\n",
    "    \n",
    "    neighborhood_sparse_features = non_dynamic_df.loc[:, neighborhood]\n",
    "    neighborhood_dynamic_features = dynamic_df.loc[:, neighborhood]\n",
    "    neighborhood_labels = theft_df.loc[:, neighborhood]\n",
    "\n",
    "    print('\\n\\nResults for {}:'.format(neighborhood))\n",
    "    print('Frequency of thefts in {}: {}'.format(neighborhood, Counter(neighborhood_labels)))\n",
    "    \n",
    "    # The research paper uses a 50%/50% balanced set for the model\n",
    "    # so I will make apply under-sampling techniques and drop some no-crime data.           \n",
    "    theft_keep_indexes = np.where(neighborhood_labels==1)[0]\n",
    "    non_theft_indexes = random.sample(population=list(np.where(neighborhood_labels==0)[0]), k=sum(neighborhood_labels==1))\n",
    "\n",
    "    # Get the indexes of the rows that I will keep for model training.\n",
    "    X_indexes = np.concatenate((non_theft_indexes, theft_keep_indexes), axis=0)\n",
    "    # sort X_indexes\n",
    "    X_indexes.sort()\n",
    "    \n",
    "    sparse_X = neighborhood_sparse_features.iloc[X_indexes]\n",
    "    dynamic_X = neighborhood_dynamic_features.iloc[X_indexes]\n",
    "    Y = neighborhood_labels.iloc[X_indexes]\n",
    "    \n",
    "    print('Frequency of thefts in {} after under-sampling: {}'.format(neighborhood, Counter(Y)))\n",
    "\n",
    "    # Secured random selection which will be used to make\n",
    "    # comparisons between sparse, dynamic, and scaled features.\n",
    "    random_seed = randbelow(4294967000)\n",
    "    X_train_sparse, X_test_sparse, Y_train, Y_test = train_test_split(sparse_X, Y, test_size=0.30, random_state=random_seed)\n",
    "    X_train_dynamic, X_test_dynamic, Y_train, Y_test = train_test_split(dynamic_X, Y, test_size=0.30, random_state=random_seed)\n",
    "\n",
    "    ## Making scaled sparse features (whole dataset)\n",
    "    scalar = StandardScaler().fit(sparse_X)\n",
    "    sparse_X_scaled = scalar.transform(sparse_X)\n",
    "\n",
    "    # sparse scaled training and test set\n",
    "    scalar = StandardScaler().fit(X_train_sparse)\n",
    "    X_train_sparse_scaled = scalar.transform(X_train_sparse)\n",
    "    X_test_sparse_scaled = scalar.transform(X_test_sparse)\n",
    "\n",
    "    ### Making scaled dynamic features (whole dataset)\n",
    "    scalar = StandardScaler().fit(dynamic_X)\n",
    "    dynamic_X_scaled = scalar.transform(dynamic_X)\n",
    "\n",
    "    # dynamic scaled training and test set\n",
    "    scalar = StandardScaler().fit(X_train_dynamic)\n",
    "    X_train_dynamic_scaled = scalar.transform(X_train_dynamic)\n",
    "    X_test_dynamic_scaled = scalar.transform(X_test_dynamic)\n",
    "    \n",
    "    print('\\nCross Validation Scores were taken from original sparse features and dynamic features to ')\n",
    "    print('evaluate dynamic feature performance. The features were also scaled to check if scaling helped performance.')\n",
    "    print('The order of features used for cross-validation are as follows:') \n",
    "    print('Sparse, Dynamic, Sparse scaled, Dynamic scaled')\n",
    "    print('\\n')\n",
    "\n",
    "    ### Modelling\n",
    "    ## SVM\n",
    "    # Sparse vs dynamic\n",
    "    svm_sparse_pred, svm_sparse_auc, svm_sparse_f1, accuracy_sparse_svm = svm_evaluation(sparse_X, Y, X_train_sparse, X_test_sparse, Y_train, Y_test)\n",
    "    svm_pred, svm_auc, svm_f1, accuracy_svm =svm_evaluation(dynamic_X, Y, X_train_dynamic, X_test_dynamic, Y_train, Y_test)\n",
    "\n",
    "    # Sparse scaled vs dynamic scaled\n",
    "    svm_sparse_pred_s, svm_sparse_auc_s, svm_sparse_f1_s, accuracy_sparse_svm_s =svm_evaluation(sparse_X_scaled, Y, X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test)\n",
    "    svm_pred_s, svm_auc_s, svm_f1_s, accuracy_svm_s = svm_evaluation(dynamic_X_scaled, Y, X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test)\n",
    "    print('')\n",
    "\n",
    "    ## Logistic Regression\n",
    "    # Sparse vs dynamic\n",
    "    lr_sparse_pred, lr_sparse_auc, lr_sparse_f1, accuracy_sparse_lr = lr_evaluation(sparse_X, Y, X_train_sparse, X_test_sparse, Y_train, Y_test)\n",
    "    lr_pred, lr_auc, lr_f1, accuracy_lr =lr_evaluation(dynamic_X, Y, X_train_dynamic, X_test_dynamic, Y_train, Y_test)\n",
    "\n",
    "    # Sparse scaled vs dynamic scaled\n",
    "    lr_sparse_pred_s, lr_sparse_auc_s, lr_sparse_f1_s, accuracy_sparse_lr_s = lr_evaluation(sparse_X_scaled, Y, X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test)\n",
    "    lr_pred_s, lr_auc_s, lr_f1_s, accuracy_lr_s = lr_evaluation(dynamic_X_scaled, Y, X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test)\n",
    "    print('')\n",
    "\n",
    "    ## Random Forest\n",
    "    # Sparse vs dynamic\n",
    "    rf_sparse_pred, rf_sparse_auc, rf_sparse_f1, accuracy_sparse_rf =RF_evaluation(sparse_X, Y, X_train_sparse, X_test_sparse, Y_train, Y_test)\n",
    "    rf_pred, rf_auc, rf_f1, accuracy_rf =RF_evaluation(dynamic_X, Y, X_train_dynamic, X_test_dynamic, Y_train, Y_test)\n",
    "\n",
    "    # Sparse scaled vs dynamic scaled\n",
    "    rf_sparse_pred_s, rf_sparse_auc_s, rf_sparse_f1_s, accuracy_sparse_rf_s =RF_evaluation(sparse_X_scaled, Y, X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test)\n",
    "    rf_pred_s, rf_auc_s, rf_f1_s, accuracy_rf_s = RF_evaluation(dynamic_X_scaled, Y, X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test)\n",
    "    \n",
    "    \n",
    "    ## Neural Network\n",
    "    # Need to get values and reshape Y_train to get this to work.\n",
    "    Y_train = Y_train.values.reshape(-1,1)\n",
    "    nn_sparse_pred_s, nn_sparse_auc_s, nn_sparse_f1_s, accuracy_sparse_nn_s = nn_evaluation(X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test, learning_rate=0.01, sigmoid_thresh=0.5, epochs=100, print_cost=False)\n",
    "    nn_pred_s, nn_auc_s, nn_f1_s, accuracy_nn_s = nn_evaluation(X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test, learning_rate=0.01, sigmoid_thresh=0.5, epochs=100, print_cost=False)\n",
    "\n",
    "\n",
    "    print('\\n\\n\\n\\nTest Set results.  *note: The cross-validation scores above give are a better indicator for evaluating')\n",
    "    print('the overall comparison performace between sparse and dynamic features.')  \n",
    "    print('')  \n",
    "    print('Sparse Features (Original) \\tDynmaic Features')\n",
    "    print('')\n",
    "    print('SVM')\n",
    "    print(' non-scaled  scaled \\t\\tnon-scaled  scaled')\n",
    "    print('auc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(svm_sparse_auc, svm_sparse_auc_s, svm_auc, svm_auc_s))\n",
    "    print(' f1: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(svm_sparse_f1, svm_sparse_f1_s, svm_f1, svm_f1_s))\n",
    "    print('acc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(accuracy_sparse_svm, accuracy_sparse_svm_s, accuracy_svm, accuracy_svm_s))\n",
    "\n",
    "    print('\\nLR')\n",
    "    print(' non-scaled  scaled \\t\\tnon-scaled  scaled')\n",
    "    print('auc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(lr_sparse_auc, lr_sparse_auc_s, lr_auc, lr_auc_s))\n",
    "    print(' f1: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(lr_sparse_f1, lr_f1_s, lr_f1,lr_f1_s))\n",
    "    print('acc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(accuracy_sparse_lr, accuracy_sparse_lr_s, accuracy_lr, accuracy_lr_s))\n",
    "\n",
    "    print('\\nRF')\n",
    "    print('non-scaled   scaled \\t\\tnon-scaled  scaled')\n",
    "    print('auc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(rf_sparse_auc, rf_sparse_auc_s, rf_auc,rf_auc_s))\n",
    "    print(' f1: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(rf_sparse_f1, rf_sparse_f1_s, rf_f1,rf_f1_s))\n",
    "    print('acc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(accuracy_sparse_rf, accuracy_sparse_rf_s, accuracy_rf, accuracy_rf_s))\n",
    "    \n",
    "    print('\\nNN. Both are scaled for gradient descent.')\n",
    "    print('             scaled   \\t\\t            scaled')\n",
    "    print('auc:         {:0.4f} \\t\\t            {:0.4f}'.format(nn_sparse_auc_s, nn_auc_s))\n",
    "    print(' f1:         {:0.4f} \\t\\t            {:0.4f}'.format(nn_sparse_f1_s, nn_f1_s))\n",
    "    print('acc:         {:0.4f} \\t\\t            {:0.4f}'.format(accuracy_sparse_nn_s, accuracy_nn_s))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_evaluation('West Village')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n\\nMachine Learning Optimization info for matrices:')\n",
    "print('Shape of features matrix (timestamp, features) : {}'.format(dynamic_df.shape))\n",
    "print('Latent dimension K = {}'.format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion: After playing around with the parameters, the matrix factorization does improve the performace of a sparse matrix when looking at the cross validation results. The performance seemed to work best when the matrix factorization was kept at much less iterations than needed for convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
