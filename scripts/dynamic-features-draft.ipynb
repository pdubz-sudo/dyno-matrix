{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "import shapefile\n",
    "from collections import Counter\n",
    "from shapely.geometry import shape, Point\n",
    "from matrix_factorization import *\n",
    "from model_evaluations import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install geopandas didn't work and neither did the conda forge version\n",
    "for some reason. I used this instead.\n",
    "https://geoffboeing.com/2014/09/using-geopandas-windows/.\n",
    "\n",
    "\n",
    "1. make a virtual environment with the whole anaconda: conda create --name geoproject anaconda.\n",
    "2. Install wheels as so without having to do the PATH part.\n",
    "\n",
    "3. Install shapafile with pip install pyshp. \n",
    "\n",
    "Geopandas is able to read all the gis files and you can change the coordinate reference system easily from it. http://matthewrocklin.com/blog/work/2017/09/21/accelerating-geopandas-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Foursquare data from Kaggle. Categorize coordinates into their respective NYC borough by using boundary polygons from https://geo.nyu.edu/catalog/nyu-2451-34563. Coordinates not in any of these polygons are in New Jersey and will be removed.\n",
    "\n",
    "The newly made dataframe will be saved as a pickle for fast data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # It will take a 1-2 minutes to load because of the data parser.\n",
    "# raw_fs = pd.read_csv(r'./data/original_data/dataset_TSMC2014_NYC.csv', parse_dates=['utcTimestamp'])\n",
    "# # Make a coordinate tuple. This is the format that compatible with the shape file read by geopandas.\n",
    "# raw_fs['coordinates'] = list(zip(raw_fs.loc[:,'longitude'], raw_fs.loc[:, 'latitude']))\n",
    "\n",
    "# boroughs = geopandas.read_file(r'.\\nyc_polygons\\nyu_2451_34510.shp').to_crs({'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create polygon boundry objects for each borough. Needed for the function below.\n",
    "# bronx = boroughs['geometry'][0]\n",
    "# brooklyn = boroughs['geometry'][1]\n",
    "# manhatten = boroughs['geometry'][2]\n",
    "# queens = boroughs['geometry'][3]\n",
    "# staten_island = boroughs['geometry'][4]\n",
    "\n",
    "\n",
    "\n",
    "# def find_borough(coord): \n",
    "#     '''Takes coordinate point and returns which nyc borough it belongs to. The boundaries are not\n",
    "#     an argument in the function so their objects must be created before the function is called.\n",
    "    \n",
    "#     Arguments:\n",
    "#     coord -- tuple, coordinates in format (longitude, latitude).\n",
    "    \n",
    "#     Return -- str, the coordinated borough.'''\n",
    "#     # Turn coordinate into a shapely point that can be checked if it's in a polygon.\n",
    "#     point = Point(coord)\n",
    "#     if bronx.contains(point) == True:\n",
    "#         return 'bronx'\n",
    "#     elif brooklyn.contains(point) == True:\n",
    "#         return 'brooklyn'\n",
    "#     elif manhatten.contains(point) == True:\n",
    "#         return 'manhatten'\n",
    "#     elif queens.contains(point) == True:\n",
    "#         return 'queens'\n",
    "#     elif staten_island.contains(point) == True:\n",
    "#         return 'staten_island'\n",
    "#     else:\n",
    "#         return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create column with coordinates borough.\n",
    "# raw_fs['borough'] = [find_borough(x) for x in raw_fs['coordinates']]\n",
    "\n",
    "\n",
    "# # There are none's because they are in New Jersey. Take them out.\n",
    "# df = raw_fs[raw_fs['borough']!='other']\n",
    "\n",
    "# # Turn borough's column into one-hots.\n",
    "# df = pd.concat([df, pd.get_dummies(df['borough'])], axis=1)\n",
    "\n",
    "# # Make space for memory.\n",
    "# del raw_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe as pickle so you don't have to re-run all the code above again.\n",
    "# df.to_pickle(r'.\\data\\cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r'.\\data\\cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2547 3-hour time intervals that have at least regional datapoint in this dataset. If there are no events in the time intervals when grouping, the new dataframe will skip over the no-event time interval. Since I have many dynamic features that could possibly have different missing data, I will keep doing a \"left\" merge to the base 2547 rows 3-hour time interval frame to I have one consistent dimension shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2547, 2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = df.groupby(pd.Grouper(key='utcTimestamp', freq='3H'))['userId'].count().reset_index().rename(columns={'userId': 'count'})\n",
    "base_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor Count: Number of unique user in region r at time interval t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time intervals:  2547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bronx</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>manhatten</th>\n",
       "      <th>queens</th>\n",
       "      <th>staten_island</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-03 18:00:00</th>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-03 21:00:00</th>\n",
       "      <td>8.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 00:00:00</th>\n",
       "      <td>13.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 03:00:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 06:00:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bronx  brooklyn  manhatten  queens  staten_island\n",
       "utcTimestamp                                                          \n",
       "2012-04-03 18:00:00    8.0      23.0       84.0    16.0            1.0\n",
       "2012-04-03 21:00:00    8.0      36.0      152.0    27.0            2.0\n",
       "2012-04-04 00:00:00   13.0      30.0      108.0    16.0            4.0\n",
       "2012-04-04 03:00:00    4.0      20.0       32.0    13.0            1.0\n",
       "2012-04-04 06:00:00    2.0       9.0        8.0     5.0            1.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitor = df.groupby([pd.Grouper(key='utcTimestamp', freq='3H'),\n",
    "                      'borough'])['userId'].nunique().unstack(level=1).reset_index()\n",
    "visitor_count = pd.merge(base_df, visitor, on='utcTimestamp', how='left').drop('count', axis=1).set_index('utcTimestamp')\n",
    "\n",
    "# Make space for memory\n",
    "del visitor\n",
    "\n",
    "print('Number of time intervals: ', visitor_count.shape[0])\n",
    "visitor_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Frequency: Number of check-ins in region r and its neighborhood at        time interval t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time intervals:  2547\n"
     ]
    }
   ],
   "source": [
    "obs = df.groupby([pd.Grouper(key='utcTimestamp', freq='3H'),\n",
    "                  'borough'])['userId'].count().unstack(level=1).reset_index()\n",
    "obs_freq = pd.merge(base_df, obs, on='utcTimestamp', how='left').drop('count', axis=1).set_index('utcTimestamp')\n",
    "\n",
    "# Make space for memory\n",
    "del obs\n",
    "\n",
    "print('Number of time intervals: ', obs_freq.shape[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor entropy: Diversity of visitors in a location with respect to their visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "borough  utcTimestamp         userId\n",
       "bronx    2012-04-03 18:00:00  267       3\n",
       "                              445       2\n",
       "                              562       3\n",
       "                              573       1\n",
       "                              620       1\n",
       "Name: venueId, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You have do merge the user ids total number of visits to the region and then divide the \n",
    "# 2 columns to get the probability that they will be there at that time.\n",
    "visits_r_t = df.groupby(['borough', pd.Grouper(key='utcTimestamp',\n",
    "                                             freq='3H'), 'userId']).count().iloc[:,0]\n",
    "\n",
    "visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId\n",
       "1    106\n",
       "2    150\n",
       "3    117\n",
       "4    173\n",
       "5     28\n",
       "Name: venueId, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of checkins per user\n",
    "total_user_checkins =df.groupby(['userId']).count().iloc[:,0]\n",
    "total_user_checkins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>reg_time_visits</th>\n",
       "      <th>total_visits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-03 18:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-04 03:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-04 12:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>2</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-05 12:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>2</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-07 18:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>4</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  borough        utcTimestamp  userId  reg_time_visits  total_visits\n",
       "0   bronx 2012-04-03 18:00:00     267                3           497\n",
       "1   bronx 2012-04-04 03:00:00     267                3           497\n",
       "2   bronx 2012-04-04 12:00:00     267                2           497\n",
       "3   bronx 2012-04-05 12:00:00     267                2           497\n",
       "4   bronx 2012-04-07 18:00:00     267                4           497"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join on user id and now you have userid number of checkins in that time interval and \n",
    "# a column that shows their total checkins.\n",
    "df_visits_r_t = pd.merge(visits_r_t.reset_index(), total_user_checkins.reset_index(),\n",
    "                   on=['userId']).rename(columns={'venueId_x': 'reg_time_visits',\n",
    "                                                  'venueId_y': 'total_visits'})\n",
    "\n",
    "# Make room for memory\n",
    "del visits_r_t, total_user_checkins\n",
    "\n",
    "df_visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>reg_time_visits</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-03 18:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>0.006036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-04 03:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>0.006036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-04 12:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>2</td>\n",
       "      <td>497</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-05 12:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>2</td>\n",
       "      <td>497</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-07 18:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>4</td>\n",
       "      <td>497</td>\n",
       "      <td>0.008048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  borough        utcTimestamp  userId  reg_time_visits  total_visits  \\\n",
       "0   bronx 2012-04-03 18:00:00     267                3           497   \n",
       "1   bronx 2012-04-04 03:00:00     267                3           497   \n",
       "2   bronx 2012-04-04 12:00:00     267                2           497   \n",
       "3   bronx 2012-04-05 12:00:00     267                2           497   \n",
       "4   bronx 2012-04-07 18:00:00     267                4           497   \n",
       "\n",
       "   probability  \n",
       "0     0.006036  \n",
       "1     0.006036  \n",
       "2     0.004024  \n",
       "3     0.004024  \n",
       "4     0.008048  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the probability that user will be at region at time interval.\n",
    "df_visits_r_t['probability'] = df_visits_r_t['reg_time_visits']/df_visits_r_t['total_visits']\n",
    "df_visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>reg_time_visits</th>\n",
       "      <th>total_visits</th>\n",
       "      <th>probability</th>\n",
       "      <th>used_for_visitor_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-03 18:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-04 03:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>3</td>\n",
       "      <td>497</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>0.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-04 12:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>2</td>\n",
       "      <td>497</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.032021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-05 12:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>2</td>\n",
       "      <td>497</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.032021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bronx</td>\n",
       "      <td>2012-04-07 18:00:00</td>\n",
       "      <td>267</td>\n",
       "      <td>4</td>\n",
       "      <td>497</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.055993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  borough        utcTimestamp  userId  reg_time_visits  total_visits  \\\n",
       "0   bronx 2012-04-03 18:00:00     267                3           497   \n",
       "1   bronx 2012-04-04 03:00:00     267                3           497   \n",
       "2   bronx 2012-04-04 12:00:00     267                2           497   \n",
       "3   bronx 2012-04-05 12:00:00     267                2           497   \n",
       "4   bronx 2012-04-07 18:00:00     267                4           497   \n",
       "\n",
       "   probability  used_for_visitor_entropy  \n",
       "0     0.006036                  0.044500  \n",
       "1     0.006036                  0.044500  \n",
       "2     0.004024                  0.032021  \n",
       "3     0.004024                  0.032021  \n",
       "4     0.008048                  0.055993  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visits_r_t['used_for_visitor_entropy'] = [-x*np.log2(x) for x in df_visits_r_t['probability']]\n",
    "df_visits_r_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time intervals:  2547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bronx</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>manhatten</th>\n",
       "      <th>queens</th>\n",
       "      <th>staten_island</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-03 18:00:00</th>\n",
       "      <td>0.425683</td>\n",
       "      <td>0.961257</td>\n",
       "      <td>4.793225</td>\n",
       "      <td>0.882073</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-03 21:00:00</th>\n",
       "      <td>0.388097</td>\n",
       "      <td>1.847471</td>\n",
       "      <td>8.960947</td>\n",
       "      <td>1.217148</td>\n",
       "      <td>0.163509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 00:00:00</th>\n",
       "      <td>0.607095</td>\n",
       "      <td>1.332356</td>\n",
       "      <td>6.275626</td>\n",
       "      <td>0.628272</td>\n",
       "      <td>0.201799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 03:00:00</th>\n",
       "      <td>0.127713</td>\n",
       "      <td>0.883753</td>\n",
       "      <td>1.768109</td>\n",
       "      <td>0.646126</td>\n",
       "      <td>0.048713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 06:00:00</th>\n",
       "      <td>0.112601</td>\n",
       "      <td>0.434975</td>\n",
       "      <td>0.337310</td>\n",
       "      <td>0.231435</td>\n",
       "      <td>0.028287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bronx  brooklyn  manhatten    queens  staten_island\n",
       "utcTimestamp                                                               \n",
       "2012-04-03 18:00:00  0.425683  0.961257   4.793225  0.882073       0.041809\n",
       "2012-04-03 21:00:00  0.388097  1.847471   8.960947  1.217148       0.163509\n",
       "2012-04-04 00:00:00  0.607095  1.332356   6.275626  0.628272       0.201799\n",
       "2012-04-04 03:00:00  0.127713  0.883753   1.768109  0.646126       0.048713\n",
       "2012-04-04 06:00:00  0.112601  0.434975   0.337310  0.231435       0.028287"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum all the individual user entropys for region r and time interval t.\n",
    "incomplete_entropy_df = df_visits_r_t.groupby(['borough', \n",
    "                                    'utcTimestamp'])['used_for_visitor_entropy'].sum().unstack(level=0).reset_index()\n",
    "\n",
    "visitor_entropy = pd.merge(base_df, incomplete_entropy_df, \n",
    "                           how='left', on='utcTimestamp').drop('count', axis=1).set_index('utcTimestamp')\n",
    "\n",
    "# Make space in memory.\n",
    "del incomplete_entropy_df\n",
    "\n",
    "print('Number of time intervals: ', visitor_entropy.shape[0])\n",
    "visitor_entropy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Region Popularity: Assess popularity of region r at time interval t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time intervals:  2547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bronx</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>manhatten</th>\n",
       "      <th>queens</th>\n",
       "      <th>staten_island</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-03 18:00:00</th>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.004902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-03 21:00:00</th>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.165698</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.119186</td>\n",
       "      <td>0.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 00:00:00</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 03:00:00</th>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.017241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 06:00:00</th>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bronx  brooklyn  manhatten    queens  staten_island\n",
       "utcTimestamp                                                               \n",
       "2012-04-03 18:00:00  0.068627  0.176471   0.622549  0.127451       0.004902\n",
       "2012-04-03 21:00:00  0.029070  0.165698   0.674419  0.119186       0.011628\n",
       "2012-04-04 00:00:00  0.060000  0.188000   0.636000  0.088000       0.028000\n",
       "2012-04-04 03:00:00  0.051724  0.258621   0.413793  0.258621       0.017241\n",
       "2012-04-04 06:00:00  0.061224  0.469388   0.326531  0.102041       0.040816"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many total checkins at all time intervals \n",
    "all_checkins_at_time = df.groupby(pd.Grouper(key='utcTimestamp', freq='3H'))['userId'].count()\n",
    "\n",
    "# Divide regions checkins at time interval by total checkins at that time interval.\n",
    "region_popularity = obs_freq.divide(all_checkins_at_time, axis=0)\n",
    "\n",
    "\n",
    "del all_checkins_at_time\n",
    "\n",
    "print('Number of time intervals: ', region_popularity.shape[0])\n",
    "region_popularity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitor Ratio:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group, count, and regroup, order by descending and take top 4\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Groupby 'userId' and 'venueId'. Count to see how many times a user checked in that venueId. This will make a series.\n",
    "user_venue_count = df.groupby(['userId', 'venueId']).count().iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that you have the count for venue checkins by each user you will re-group by 'userId' and apply a function \n",
    "# to each 'userId' grouping (sort descending and keep top 4) to get back the top 4 visited venues for that user.\n",
    "user_grouping = user_venue_count.groupby(level=0, group_keys=False)  # When calling apply, add group keys argument to prevent key appearing twice IT.\n",
    "user_top_venues = user_grouping.apply(lambda x: x.sort_values(ascending=False).head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time intervals:  2547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bronx</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>manhatten</th>\n",
       "      <th>queens</th>\n",
       "      <th>staten_island</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-03 18:00:00</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.385827</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-03 21:00:00</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.254310</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 00:00:00</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 03:00:00</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 06:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bronx  brooklyn  manhatten    queens  staten_island\n",
       "utcTimestamp                                                               \n",
       "2012-04-03 18:00:00  0.357143  0.277778   0.385827  0.346154       1.000000\n",
       "2012-04-03 21:00:00  0.500000  0.298246   0.254310  0.341463       0.500000\n",
       "2012-04-04 00:00:00  0.533333  0.276596   0.226415  0.363636       0.428571\n",
       "2012-04-04 03:00:00  0.166667  0.400000   0.229167  0.300000            NaN\n",
       "2012-04-04 06:00:00       NaN  0.304348   0.437500  0.400000            NaN"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need the timestamp and borough of all the users top 4 checkins.\n",
    "top_4_full_df = pd.merge(user_top_venues.reset_index(), df, how='left', on=['userId', 'venueId'])\n",
    "\n",
    "# Drop duplicates so I get the first visit of the users top 4 checkins.\n",
    "first_visit = top_4_full_df.drop_duplicates(['userId', 'venueId'])\n",
    "incomplete_visit_df = first_visit.groupby([pd.Grouper(key='utcTimestamp', freq='3H'), 'borough']).count().iloc[:, 0].unstack(level=1)\n",
    "\n",
    "##################### SANITY CHECK\n",
    "# # All that is commented out, to make sure the time stamps are ascending since I will drop duplicates.\n",
    "# # It seems like the timestamps are in order but just in case they are not I will order them because I\n",
    "# # am going to remove the duplicates based on userId and venueId so it will only keep the first occurence\n",
    "# # which gives up the first checkin timestamp.\n",
    "# sort_df = top_4_full_df.set_index(['userId', 'venueId']).groupby(level=[0,1], group_keys=False).apply(lambda x: x.sort_values(by='utcTimestamp'))\n",
    "# first_visit = sort_df.reset_index().drop_duplicates(['userId', 'venueId'])\n",
    "\n",
    "# # Group by time stamp and borough. Take a count and unpivot the column so that we have the \n",
    "# # have the first time occurences of the top 4 userid visits by venue. The rows containing all zeros are excluded so\n",
    "# # this needs to be joined back to the base df so we have a row for each time interval.\n",
    "# incomplete_visit_df = first_visit.groupby([pd.TimeGrouper(key='utcTimestamp', freq='3H'), 'borough']).count().iloc[:, 0].unstack(level=1)\n",
    "######################\n",
    "\n",
    "# Join\n",
    "new_user_venue_checkins = pd.merge(base_df, incomplete_visit_df.reset_index(), how='left', on='utcTimestamp').drop('count', axis=1)\n",
    "\n",
    "# Calculate visitor ratio by dividing by observation frequency for that region at that time interval.\n",
    "visitor_ratio = new_user_venue_checkins.set_index('utcTimestamp')/obs_freq\n",
    "\n",
    "# Make space for memory.\n",
    "del user_venue_count, user_grouping, user_top_venues, top_4_full_df\n",
    "del first_visit,incomplete_visit_df, new_user_venue_checkins\n",
    "\n",
    "print('Number of time intervals: ', visitor_ratio.shape[0])\n",
    "visitor_ratio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All dynamic features done except for one of them. Concat them all together, do the matrix factorization (don't forget to fillna(0), and take features for the bronx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_df = pd.concat([visitor_count, \n",
    "           obs_freq,\n",
    "           visitor_entropy,\n",
    "           visitor_ratio,\n",
    "           region_popularity], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove cities that have low observation frequency from concatentated df since they don't have much\n",
    "# meaningful information. Remove cities with a mean observation frequency of less than 15.\n",
    "low_checkin_cities = [city for city in obs_freq.columns if obs_freq[city].mean() < 15]\n",
    "\n",
    "all_features_df.drop(low_checkin_cities, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove time intervals that have low activity as well. If a time interval doesn't have \n",
    "# 10*(number of regions) in the df then it will be dropped.\n",
    "number_regions = all_features_df.columns.unique().shape[0]\n",
    "low_activity_hours_indexes = np.where(obs_freq.drop(low_checkin_cities, axis=1).sum(axis=1) < 10*number_regions)[0]\n",
    "\n",
    "# # Drop the low activity indexes from df.\n",
    "all_features_df.drop(all_features_df.index[tuple([low_activity_hours_indexes])], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 6921350.25359\n",
      "Loss after epoch 10: 221614.92660\n",
      "Loss after epoch 20: 174083.84404\n",
      "Loss after epoch 30: 74740.43979\n",
      "Loss after epoch 40: 34550.33034\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = all_features_df.columns\n",
    "index = all_features_df.index\n",
    "\n",
    "dynamic, _ = dynamic_feature_estimation(all_features_df.fillna(0).values, K=15, alpha=0.0002, beta=0.001, epochs=50, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To rebuild the df\n",
    "dynamic_df = pd.DataFrame(dynamic, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Brooklyn's dynamic features\n",
    "brook_dyn = dynamic_df.loc[:, 'brooklyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############333 Testing without the recommendation system\n",
    "# brook_dyn = all_features_df.loc[:, 'brooklyn'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##### There is crime data since the starting date of the dynamic features data!!!!!\n",
    "# # Many of the dates in the raw file are wrong so I couldn't parse the dates from reading\n",
    "# # in the file to correctly pre-process it the way I wanted it.\n",
    "# raw_crime_data = pd.read_csv(r'.\\data\\original_data\\NYPD_Complaint_Data_Historic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # There are years that are wrong dates such as 1015 which will throw errors.\n",
    "# # I 'coerce' the parsing and these wrong ones will turn into 'NaT'.\n",
    "# raw_crime_data['Date'] = pd.to_datetime(raw_crime_data['CMPLNT_FR_DT'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# # Do the same with time and then use 'dt.time' to get just the time. There are 24:00:00 which\n",
    "# # doesn't really exist so let's 'coerce' them to 'NaT' because I don't know if they are meant to be\n",
    "# # on the next day or the current day of the date.\n",
    "# raw_crime_data['Time'] = pd.to_datetime(raw_crime_data['CMPLNT_TO_TM'], \n",
    "#                                         format='%H:%M:%S', errors='coerce').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Drop the 'NaT''s from 'Time' and 'Date' columns\n",
    "# drop_indexes_date = np.where(raw_crime_data['Date'].isnull())[0]\n",
    "# drop_indexes_time = np.where(raw_crime_data['Time'].isnull())[0]\n",
    "\n",
    "\n",
    "# drop_list = np.concatenate((drop_indexes_date, drop_indexes_time), axis=0)\n",
    "\n",
    "# raw_crime_data.drop(drop_list, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Filter rows from range 04-03-2012 and 02-06-2013 which is our FourSquare data range.\n",
    "# raw_crime_data = raw_crime_data.set_index('Date').loc['20120403': '20130216'].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reset index to start combining 'Date', and 'Time' to make a timestamp.\n",
    "# raw_crime_data = raw_crime_data.reset_index()\n",
    "\n",
    "# # Couldn't figure out how to combine the 'Date' and 'Time' datetime objects together\n",
    "# # so I turned them back into string and combined them.\n",
    "# # I had to access 'dt' first.\n",
    "# raw_crime_data['Date'] = raw_crime_data['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# # I couldn't do this the same way as above and had to use lambda instead to get it to work.\n",
    "# raw_crime_data['Time'] = raw_crime_data['Time'].apply(lambda x: x.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_crime_data['utcTimestamp'] = pd.to_datetime(raw_crime_data['Date'] + ' ' + raw_crime_data['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Grand Larceny (theft df). Theft had a low p-value in the research paper.\n",
    "# theft_df = raw_crime_data[raw_crime_data['OFNS_DESC']=='GRAND LARCENY']\n",
    "\n",
    "# del raw_crime_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theft_group = theft_df.reset_index().groupby([pd.TimeGrouper(key='utcTimestamp', freq='3H'), 'BORO_NM']).count()\n",
    "# cleaned_theft_labels = theft_group.iloc[:,0].unstack(level=1).fillna(0)\n",
    "\n",
    "# del theft_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save as pickle for faster loading\n",
    "# pd.to_pickle(cleaned_theft_labels, r'.\\data\\cleaned_theft_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theft_df = pd.read_pickle(r'.\\data\\cleaned_theft_labels.pkl')\n",
    "\n",
    "# Get theft labels for brooklyn\n",
    "brook_labels = theft_df['BROOKLYN']\n",
    "\n",
    "# Our model prediction will be binary so turn labels to binary\n",
    "brook_labels[brook_labels>0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of thefts in Brooklyn:  Counter({1.0: 1253, 0.0: 96})\n"
     ]
    }
   ],
   "source": [
    "#############################333\n",
    "# Joining the dynamic features and labels on the time index.\n",
    "brook_df = pd.merge(brook_dyn, brook_labels.to_frame(), left_index=True, right_index=True).rename(columns={'BROOKLYN': 'labels'}).fillna(0)\n",
    "print('frequency of thefts in Brooklyn: ', Counter(brook_df['labels']))\n",
    "\n",
    "# The labels are imbalanced so I randomly make a 50%/50% label balanced dataset.\n",
    "theft_keep_indexes = random.sample(population=list(np.where(brook_df['labels']==1)[0]), k=sum(brook_df['labels']==0))\n",
    "non_theft_indexes = np.where(brook_df['labels']==0)[0]\n",
    "\n",
    "# Get the indexes of the rows that I will keep for model training.\n",
    "X_indexes = np.concatenate((non_theft_indexes, theft_keep_indexes), axis=0)\n",
    "# sort X_indexes\n",
    "X_indexes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>brooklyn</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utcTimestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-03 18:00:00</th>\n",
       "      <td>22.456320</td>\n",
       "      <td>36.455202</td>\n",
       "      <td>0.990089</td>\n",
       "      <td>1.165996</td>\n",
       "      <td>0.113670</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-03 21:00:00</th>\n",
       "      <td>34.670459</td>\n",
       "      <td>57.115800</td>\n",
       "      <td>1.666329</td>\n",
       "      <td>-1.344850</td>\n",
       "      <td>0.116877</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 00:00:00</th>\n",
       "      <td>28.679631</td>\n",
       "      <td>47.298599</td>\n",
       "      <td>1.278413</td>\n",
       "      <td>0.440533</td>\n",
       "      <td>0.083299</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 03:00:00</th>\n",
       "      <td>18.500079</td>\n",
       "      <td>31.483803</td>\n",
       "      <td>0.787625</td>\n",
       "      <td>1.935805</td>\n",
       "      <td>0.165540</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-04 06:00:00</th>\n",
       "      <td>11.981353</td>\n",
       "      <td>21.332647</td>\n",
       "      <td>0.277404</td>\n",
       "      <td>3.066621</td>\n",
       "      <td>-0.012015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      brooklyn   brooklyn  brooklyn  brooklyn  brooklyn  \\\n",
       "utcTimestamp                                                              \n",
       "2012-04-03 18:00:00  22.456320  36.455202  0.990089  1.165996  0.113670   \n",
       "2012-04-03 21:00:00  34.670459  57.115800  1.666329 -1.344850  0.116877   \n",
       "2012-04-04 00:00:00  28.679631  47.298599  1.278413  0.440533  0.083299   \n",
       "2012-04-04 03:00:00  18.500079  31.483803  0.787625  1.935805  0.165540   \n",
       "2012-04-04 06:00:00  11.981353  21.332647  0.277404  3.066621 -0.012015   \n",
       "\n",
       "                     labels  \n",
       "utcTimestamp                 \n",
       "2012-04-03 18:00:00     1.0  \n",
       "2012-04-03 21:00:00     1.0  \n",
       "2012-04-04 00:00:00     1.0  \n",
       "2012-04-04 03:00:00     0.0  \n",
       "2012-04-04 06:00:00     1.0  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brook_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brook_row_filter = brook_df.iloc[X_indexes]\n",
    "X = brook_row_filter.iloc[:, :5]\n",
    "Y = brook_row_filter.iloc[:, -1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30)#, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scalar.transform(X_train)\n",
    "X_test_scaled = scalar.transform(X_test)\n",
    "\n",
    "scalar = StandardScaler().fit(X)\n",
    "X_scaled = scalar.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "svm_evaluation() takes 4 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-37928668f67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# the distribution of randomly selected training data is influencing the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# results even more.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msvm_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_auc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_svm\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0msvm_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msvm_pred_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_auc_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_f1_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_svm_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: svm_evaluation() takes 4 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "# Scaling had mixed results. Not sure if I should scale because the bigger\n",
    "# the distribution of randomly selected training data is influencing the \n",
    "# results even more.\n",
    "svm_pred, svm_auc, svm_f1, accuracy_svm =svm_evaluation(X, Y, X_train, X_test, Y_train, Y_test)\n",
    "svm_pred_s, svm_auc_s, svm_f1_s, accuracy_svm_s = svm_evaluation(X_scaled, Y, X_train_scaled, X_test_scaled, Y_train, Y_test)\n",
    "\n",
    "lr_pred, lr_auc, lr_f1, accuracy_lr =lr_evaluation(X, Y, X_train, X_test, Y_train, Y_test)\n",
    "lr_pred_s, lr_auc_s, lr_f1_s, accuracy_lr_s = lr_evaluation(X_scaled, Y, X_train_scaled, X_test_scaled, Y_train, Y_test)\n",
    "\n",
    "rf_pred, rf_auc, rf_f1, accuracy_rf =RF_evaluation(X, Y, X_train, X_test, Y_train, Y_test)\n",
    "rf_pred_s, rf_auc_s, rf_f1_s, accuracy_rf_s = RF_evaluation(X_scaled, Y, X_train_scaled, X_test_scaled, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      " non-scaled  scaled\n",
      "auc: 0.5238, 0.5512\n",
      " f1: 0.3846, 0.3913\n",
      "acc: 0.4483, 0.5172\n",
      "\n",
      "LR\n",
      "non-scaled   scaled\n",
      "auc: 0.7241, 0.5517\n",
      " f1: 0.7143, 0.5357\n",
      "acc: 0.7241, 0.5517\n",
      "\n",
      "RF\n",
      "non-scaled   scaled\n",
      "auc: 0.5232, 0.5185\n",
      " f1: 0.4815, 0.4528\n",
      "acc: 0.5172, 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Mixed results between non-scaled and scaled features.\n",
    "print('SVM')\n",
    "print(' non-scaled  scaled')\n",
    "print('auc: {:0.4f}, {:0.4f}'.format(svm_auc, svm_auc_s))\n",
    "print(' f1: {:0.4f}, {:0.4f}'.format(svm_f1, svm_f1_s))\n",
    "print('acc: {:0.4f}, {:0.4f}'.format(accuracy_svm, accuracy_svm_s))\n",
    "\n",
    "print('\\nLR')\n",
    "print('non-scaled   scaled')\n",
    "print('auc: {:0.4f}, {:0.4f}'.format(lr_auc, lr_auc_s))\n",
    "print(' f1: {:0.4f}, {:0.4f}'.format(lr_f1,lr_f1_s))\n",
    "print('acc: {:0.4f}, {:0.4f}'.format(accuracy_lr, accuracy_lr_s))\n",
    "\n",
    "print('\\nRF')\n",
    "print('non-scaled   scaled')\n",
    "print('auc: {:0.4f}, {:0.4f}'.format(rf_auc,rf_auc_s))\n",
    "print(' f1: {:0.4f}, {:0.4f}'.format(rf_f1,rf_f1_s))\n",
    "print('acc: {:0.4f}, {:0.4f}'.format(accuracy_rf, accuracy_rf_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.66666667, 0.        , 0.66666667, 0.33333333,\n",
       "       0.        , 0.        , 0.33333333, 1.        , 0.33333333,\n",
       "       0.66666667, 0.66666667, 0.66666667, 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.66666667, 0.        , 0.66666667,\n",
       "       0.        , 0.        , 0.66666667, 0.33333333, 0.66666667,\n",
       "       0.        , 0.        , 0.33333333, 0.33333333, 0.33333333,\n",
       "       0.66666667, 0.66666667, 0.33333333, 0.33333333, 1.        ,\n",
       "       0.        , 0.33333333, 0.        , 0.33333333, 0.        ,\n",
       "       0.66666667, 0.66666667, 1.        , 0.33333333, 0.        ,\n",
       "       0.        , 0.33333333, 0.33333333, 0.33333333, 0.66666667,\n",
       "       1.        , 1.        , 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum Rule\n",
    "(svm_pred_s + lr_pred + rf_pred_s) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of thefts in BROOKLYN: Counter({1.0: 2342, 0.0: 194})\n",
      "Frequency of thefts in BROOKLYN after under-sampling: Counter({1.0: 194, 0.0: 194})\n",
      "\n",
      "\n",
      "Results:\n",
      "Dynamic Features \t\tSparse Features (Original)\n",
      "\n",
      "SVM\n",
      " non-scaled  scaled \t\tnon-scaled  scaled\n",
      "auc: 0.5928, 0.5796 \t\t    0.5618, 0.5667\n",
      " f1: 0.5000, 0.5455 \t\t    0.4040, 0.5766\n",
      "acc: 0.5726, 0.5726 \t\t    0.4957, 0.5983\n",
      "\n",
      "LR\n",
      " non-scaled  scaled \t\tnon-scaled  scaled\n",
      "auc: 0.4957, 0.5299 \t\t    0.5299, 0.5128\n",
      " f1: 0.4158, 0.4660 \t\t    0.3529, 0.4660\n",
      "acc: 0.4957, 0.5299 \t\t    0.5299, 0.5128\n",
      "\n",
      "RF\n",
      "non-scaled   scaled\n",
      "auc: 0.5340, 0.5417 \t\t    0.6111, 0.6126\n",
      " f1: 0.5124, 0.5620 \t\t    0.5370, 0.5000\n",
      "acc: 0.4957, 0.5470 \t\t    0.5726, 0.5556\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Testing some stuff out. If you take out low checkin cities and low checkin times \n",
    "# Then you can have higher results compares to just keeping everything.\n",
    "from secrets import randbelow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from model_evaluations_v2 import *\n",
    "\n",
    "\n",
    "all_features_df = all_features_df.fillna(0)\n",
    "full_dynamic_df = pd.DataFrame(dynamic, columns=columns, index=index)\n",
    "theft_df = pd.read_pickle(r'.\\data\\cleaned_theft_labels.pkl')\n",
    "\n",
    "# The author said the crime labels are binary so I make them binary.\n",
    "theft_df[theft_df>1]=1\n",
    "\n",
    "## Sanity check\n",
    "# np.where(np.asarray(theft_df)>1)\n",
    "\n",
    "# Get the time indexes that both X and Y df's match on, 'inner', so\n",
    "# I can start deciding which city I will run the model and can then actually\n",
    "# build the model.\n",
    "timestamp_indexes_keep = pd.merge(full_dynamic_df, theft_df,\n",
    "                                  on='utcTimestamp').index\n",
    "\n",
    "non_dynamic_df = all_features_df.loc[timestamp_indexes_keep]\n",
    "dynamic_df = full_dynamic_df.loc[timestamp_indexes_keep]\n",
    "theft_df = theft_df.loc[timestamp_indexes_keep]\n",
    "\n",
    "def neighborhood_evalution(neighborhood, neighborhood2):\n",
    "    '''The neighborhoods features are used to evaluate theft predictions with \n",
    "    SVM, LR, and Random Forest and to compare original sparse features vs \n",
    "    it's dynamic features. The evaluation metrics are auc, f1, and accuracy.\n",
    "    The results also compare to see if scaling the features improves results. \n",
    "    This function can be easily changed to return predictions, auc, f1, and \n",
    "    accuracy for all the differentcombinations of sparse, dynamic, \n",
    "    and scaled training data.\n",
    "    \n",
    "    Arguments:\n",
    "    neighborhood -- str, neighborhood name located in columns dataframe.\n",
    "    \n",
    "    Return:\n",
    "    None\n",
    "    '''\n",
    "    \n",
    "    neighborhood_sparse_features = non_dynamic_df.loc[:, neighborhood2]\n",
    "    neighborhood_dynamic_features = dynamic_df.loc[:, neighborhood2]\n",
    "    neighborhood_labels = theft_df.loc[:, neighborhood]\n",
    "\n",
    "    print('Frequency of thefts in {}: {}'.format(neighborhood, Counter(neighborhood_labels)))\n",
    "    \n",
    "    # The research paper uses a 50%/50% balanced set for the model\n",
    "    # so I will make apply under-sampling techniques and drop some no-crime data.           \n",
    "    theft_keep_indexes = np.where(neighborhood_labels==0)[0]\n",
    "    non_theft_indexes = random.sample(population=list(np.where(neighborhood_labels==1)[0]), k=sum(neighborhood_labels==0))\n",
    "\n",
    "    # Get the indexes of the rows that I will keep for model training.\n",
    "    X_indexes = np.concatenate((non_theft_indexes, theft_keep_indexes), axis=0)\n",
    "    # sort X_indexes\n",
    "    X_indexes.sort()\n",
    "    \n",
    "    sparse_X = neighborhood_sparse_features.iloc[X_indexes]\n",
    "    dynamic_X = neighborhood_dynamic_features.iloc[X_indexes]\n",
    "    Y = neighborhood_labels.iloc[X_indexes]\n",
    "    \n",
    "    print('Frequency of thefts in {} after under-sampling: {}'.format(neighborhood, Counter(Y)))\n",
    "\n",
    "    # Secured random selection which will be used to make\n",
    "    # comparisons between sparse, dynamic, and scaled features.\n",
    "    random_seed = randbelow(4294967000)\n",
    "    X_train_sparse, X_test_sparse, Y_train, Y_test = train_test_split(sparse_X, Y, test_size=0.30, random_state=random_seed)\n",
    "    X_train_dynamic, X_test_dynamic, Y_train, Y_test = train_test_split(dynamic_X, Y, test_size=0.30, random_state=random_seed)\n",
    "\n",
    "    # Making scaled sparse features\n",
    "    scalar = StandardScaler().fit(X_train_sparse)\n",
    "    X_train_sparse_scaled = scalar.transform(X_train_sparse)\n",
    "    X_test_sparse_scaled = scalar.transform(X_test_sparse)\n",
    "\n",
    "    # Making scaled dynamic features\n",
    "    scalar = StandardScaler().fit(X_train_dynamic)\n",
    "    X_train_dynamic_scaled = scalar.transform(X_train_dynamic)\n",
    "    X_test_dynamic_scaled = scalar.transform(X_test_dynamic)\n",
    "    \n",
    "    ### Modelling\n",
    "    ## SVM\n",
    "    # Sparse\n",
    "    svm_sparse_pred, svm_sparse_auc, svm_sparse_f1, accuracy_sparse_svm = svm_evaluation(X_train_sparse, X_test_sparse, Y_train, Y_test)\n",
    "    svm_sparse_pred_s, svm_sparse_auc_s, svm_sparse_f1_s, accuracy_sparse_svm_s =svm_evaluation(X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test)\n",
    "    # Dynamic\n",
    "    svm_pred, svm_auc, svm_f1, accuracy_svm =svm_evaluation(X_train_dynamic, X_test_dynamic, Y_train, Y_test)\n",
    "    svm_pred_s, svm_auc_s, svm_f1_s, accuracy_svm_s = svm_evaluation(X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test)\n",
    "    \n",
    "    ## Logistic Regression\n",
    "    # Sparse\n",
    "    lr_sparse_pred, lr_sparse_auc, lr_sparse_f1, accuracy_sparse_lr = lr_evaluation(X_train_sparse, X_test_sparse, Y_train, Y_test)\n",
    "    lr_sparse_pred_s, lr_sparse_auc_s, lr_sparse_f1_s, accuracy_sparse_lr_s = lr_evaluation(X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test)\n",
    "    # Dynamic\n",
    "    lr_pred, lr_auc, lr_f1, accuracy_lr =lr_evaluation(X_train_dynamic, X_test_dynamic, Y_train, Y_test)\n",
    "    lr_pred_s, lr_auc_s, lr_f1_s, accuracy_lr_s = lr_evaluation(X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test)\n",
    "    \n",
    "    ## Random Forest\n",
    "    # Sparse\n",
    "    rf_sparse_pred, rf_sparse_auc, rf_sparse_f1, accuracy_sparse_rf =RF_evaluation(X_train_sparse, X_test_sparse, Y_train, Y_test)\n",
    "    rf_sparse_pred_s, rf_sparse_auc_s, rf_sparse_f1_s, accuracy_sparse_rf_s =RF_evaluation(X_train_sparse_scaled, X_test_sparse_scaled, Y_train, Y_test)\n",
    "    \n",
    "    # Dynamic\n",
    "    rf_pred, rf_auc, rf_f1, accuracy_rf =RF_evaluation(X_train_dynamic, X_test_dynamic, Y_train, Y_test)\n",
    "    rf_pred_s, rf_auc_s, rf_f1_s, accuracy_rf_s = RF_evaluation(X_train_dynamic_scaled, X_test_dynamic_scaled, Y_train, Y_test)\n",
    "    \n",
    "    \n",
    "    print('\\n\\nResults:')\n",
    "    print('Dynamic Features \\t\\tSparse Features (Original)')\n",
    "    print('')\n",
    "    print('SVM')\n",
    "    print(' non-scaled  scaled \\t\\tnon-scaled  scaled')\n",
    "    print('auc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(svm_auc, svm_auc_s, svm_sparse_auc, svm_sparse_auc_s))\n",
    "    print(' f1: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(svm_f1, svm_f1_s, svm_sparse_f1, svm_sparse_f1_s))\n",
    "    print('acc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(accuracy_svm, accuracy_svm_s, accuracy_sparse_svm, accuracy_sparse_svm_s))\n",
    "\n",
    "    print('\\nLR')\n",
    "    print(' non-scaled  scaled \\t\\tnon-scaled  scaled')\n",
    "    print('auc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(lr_auc, lr_auc_s, lr_sparse_auc, lr_sparse_auc_s))\n",
    "    print(' f1: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(lr_f1,lr_f1_s, lr_sparse_f1, lr_f1_s))\n",
    "    print('acc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(accuracy_lr, accuracy_lr_s, accuracy_sparse_lr, accuracy_sparse_lr_s))\n",
    "\n",
    "    print('\\nRF')\n",
    "    print('non-scaled   scaled')\n",
    "    print('auc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(rf_auc,rf_auc_s, rf_sparse_auc, rf_sparse_auc_s))\n",
    "    print(' f1: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(rf_f1,rf_f1_s, rf_sparse_f1, rf_sparse_f1_s))\n",
    "    print('acc: {:0.4f}, {:0.4f} \\t\\t    {:0.4f}, {:0.4f}'.format(accuracy_rf, accuracy_rf_s, accuracy_sparse_rf, accuracy_sparse_rf_s))\n",
    "    \n",
    "    return None\n",
    "\n",
    "neighborhood_evalution('BROOKLYN', 'brooklyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "test = all_features_df.unstack().reset_index(name='value')\n",
    "test = test.rename(columns={'level_0': 'borough','0': 'value'})\n",
    "test = test.reindex(columns=['utcTimestamp', 'borough', 'value']).fillna(0)\n",
    "min_val =test.min()[2]\n",
    "max_val = test.max()[2]\n",
    "reader = Reader(line_format='user item rating', rating_scale=(min_val, max_val))\n",
    "\n",
    "data = Dataset.load_from_df(test[['utcTimestamp', 'borough', 'value']],reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo = SVD(n_factors=20, biased=False)\n",
    "algo.fit(trainingset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix1 = algo.pu\n",
    "matrix2 =algo.qi\n",
    "pd.DataFrame(np.dot(matrix1, matrix2.T)).head()\n",
    "matrix2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "all_features_df = pd.concat([visitor_count, \n",
    "           obs_freq,\n",
    "           visitor_entropy,\n",
    "           visitor_ratio,\n",
    "           region_popularity], axis=1)\n",
    "\n",
    "bronx = all_features_df.loc[:, 'bronx']\n",
    "bronx.apply(lambda x: x-bronx.mean(axis=1)).values\n",
    "\n",
    "bronx.iloc[:, :]\n",
    "new, _ = dynamic_feature_estimation(all_features_df.fillna(0).values, K=20, alpha=0.0001, beta=0.001, epochs=100, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_df.groupby('bronx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_df.loc['2013-01-01 00:00:00'::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "test = np.asarray(visitor_entropy.fillna(0))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model, _ = dynamic_feature_estimation(test, K=3, alpha=0.00001, beta=0.001, epochs=100, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_model, columns=['nyc', '2', '3', '4', '5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shapefile\n",
    "from shapely.geometry import shape, Point\n",
    "# To install shapely\n",
    "# https://gis.stackexchange.com/questions/38899/geos-and-shapely-installation-on-windows/132743#132743\n",
    "# Download wheel file for the version of python that you use.\n",
    "# https://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely\n",
    "# cd into the location of the wheel file\n",
    "# pip install wheel\n",
    "# pip install name_of_wheel_file.whl\n",
    "\n",
    "# read fileshape\n",
    "r = shapefile.Reader(r\"C:\\Users\\P\\OneDrive\\securitas-assignment\\nyu_2451_34510\\nyu_2451_34510.shp\")\n",
    "\n",
    "# get the shapes\n",
    "shapes = r.shapes()\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/7861196/check-if-a-geopoint-with-latitude-and-longitude-is-within-a-shapefile\n",
    "# build a shapely poolygon from your shape\n",
    "p1 = shape(shapes[0])\n",
    "p2 = shape(shapes[1])\n",
    "p3 = shape(shapes[2])\n",
    "p4 = shape(shapes[3])\n",
    "p5 = shape(shapes[4])\n",
    "\n",
    "test  = fs['longitude'][0], fs['latitude'][0]\n",
    "point = Point(test)\n",
    "print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visitor_ratio = generate_data()\n",
    "region_populatiry = generate_data()\n",
    "visitor_entropy = generate_data()\n",
    "visitor_homogen = generate_data()\n",
    "obser_freq = generate_data()\n",
    "visitor_count= generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visitor_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    data=np.random.randint(0, 200, (2000, 1))\n",
    "    data[(data<160) & (40<data)]=0\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
