{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "K=4\n",
    "alpha = 0.09\n",
    "beta = 0.001\n",
    "iterations = 2000\n",
    "\n",
    "time_intervals = matrix.shape[0]\n",
    "regions = matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_zero(array_2d, time_intervals):\n",
    "    '''Returns indexes(i,j) and of non-zeros and their value which will \n",
    "    be used for learning the latent features.\n",
    "    \n",
    "    Arguments:\n",
    "    array_2d -- 2D array\n",
    "    time_intervals -- int, number of rows for time interval axis of array.\n",
    "    \n",
    "    return non_zero_examples -- Returns (i, j, value)\n",
    "    '''\n",
    "    \n",
    "    non_zero_examples = []\n",
    "    for i in range(time_intervals):\n",
    "        for j in range(regions):\n",
    "            if array_2d[i, j] != 0:  ####### maybe change to >0\n",
    "                non_zero_examples.append((i, j, array_2d[i,j]))\n",
    "\n",
    "    return non_zero_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### trying taking out b\n",
    "\n",
    "def get_dynamic_feature(i, j, b, b_time_intervals, b_regions, P, Q):\n",
    "    '''Does a vector multiplication of a slice from P and Q which results in \n",
    "    a prediction for that sample from the 2d array; multiplying 2 vectors results\n",
    "    in a single value.\n",
    "    \n",
    "    i -- int, index of time_intervals axis.\n",
    "    j -- int, index of regions axis.\n",
    "    \n",
    "    Returns:\n",
    "    sample_prediction -- float, prediction of that at array_2d[i, j].'''\n",
    "    \n",
    "    sample_prediction = b \\\n",
    "    + b_time_intervals[i] \\\n",
    "    + b_regions[j] \\\n",
    "    + np.dot(P[i, :], Q[j, :].T)\n",
    "    \n",
    "    return sample_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(non_zero_examples, b, b_time_intervals, b_regions, P, Q, alpha, beta):\n",
    "    '''Performs updates using gradient descent for each example.\n",
    "    \n",
    "    Arguments:\n",
    "    non_zero_examples -- list of tuples, (i, j, r-value)\n",
    "    \n",
    "    Return:\n",
    "    Will not return anything. Instead it is updating biases and latent matrices.'''\n",
    "    for i, j, r in non_zero_examples:\n",
    "        example_pred = get_dynamic_feature(i, j, b, b_time_intervals, b_regions, P, Q)\n",
    "        example_error = r - example_pred\n",
    "        \n",
    "        # Update biases\n",
    "        b_time_intervals[i] += alpha * (example_error - beta*b_time_intervals[i])\n",
    "        b_regions[j] += alpha * (example_error - beta*b_regions[j])\n",
    "        \n",
    "        # Update time interval and region latent feature matrices\n",
    "        P[i, :] += alpha * (2*example_error*Q[j, :] - beta*P[i, :])  \n",
    "        Q[j, :] += alpha * (2*example_error*P[i, :] - beta*Q[j, :])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_features_array(b, b_time_intervals, b_regions, P, Q):\n",
    "    '''Creates dynamic features array by matrix multiplying the latent matrices\n",
    "    and adding the biases and mean.\n",
    "    \n",
    "    Arguments:\n",
    "    b -- float, mean.\n",
    "    b_time_intervals -- biases for time_intervals axis.\n",
    "    b_regions -- biases for region axis.\n",
    "    P -- 2d array, latent feature array shape(timeintervals, K).\n",
    "    Q --2d array, latene feature array shape(regions, K)\n",
    "    \n",
    "    Return:\n",
    "    Predicted_r_array -- dynamic feature matrix.'''\n",
    "    \n",
    "    predicted_r_array = b \\\n",
    "    + b_time_intervals.reshape(-1, 1) \\\n",
    "    + b_regions.reshape(1, -1) \\\n",
    "    + np.dot(P, Q.T)\n",
    "    \n",
    "    return predicted_r_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(array_2d, b, b_time_intervals, b_regions, P, Q, beta):\n",
    "    '''Sum of squares error plus reguarlization terms which will be minimized.\n",
    "    \n",
    "    Arguments:\n",
    "    array_2d -- 2d array, original array before dynamic features are added.\n",
    "    b -- float, mean.\n",
    "    b_time_intervals -- biases for time_intervals axis.\n",
    "    b_regions -- biases for region axis.\n",
    "    P -- 2d array, latent feature array shape(timeintervals, K).\n",
    "    Q --2d array, latene feature array shape(regions, K)\n",
    "    \n",
    "    Return:\n",
    "    cost -- float, cost.\n",
    "    '''\n",
    "    x_index, y_index = array_2d.nonzero()\n",
    "    pred_2d_array = dynamic_features_array(b, b_time_intervals, b_regions, P, Q)\n",
    "    squared_diff = 0\n",
    "    for x, y in zip(x_index, y_index):\n",
    "        squared_diff += (array_2d[x, y] - pred_2d_array[x, y])**2\n",
    "    reg_P = (beta/2)*sum(sum(P**2))\n",
    "    reg_Q = (beta/2)*sum(sum(Q**2))\n",
    "    cost = squared_diff/2 + reg_P + reg_Q  \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals = matrix.shape[0]\n",
    "regions = matrix.shape[1]\n",
    "\n",
    "P = np.random.normal(scale=1/K, size=(time_intervals, K))\n",
    "Q = np.random.normal(scale=1/K, size=(regions, K))\n",
    "\n",
    "\n",
    "b_time_intervals = np.zeros(time_intervals)\n",
    "b_regions = np.zeros(regions)\n",
    "b = np.mean(matrix[np.where(matrix!=0)])\n",
    "\n",
    "costs = []\n",
    "non_zero_examples = non_zero(matrix, time_intervals)\n",
    "for i in range(iterations):\n",
    "    np.random.shuffle(non_zero_examples)\n",
    "    \n",
    "    # Update latent matrices. This function will not return anything\n",
    "    # and will perform the updates from gradient descent.\n",
    "    stochastic_grad_descent(non_zero_examples)\n",
    "    cost = cost_function(matrix, b, b_time_intervals, b_regions, P, Q)\n",
    "    costs.append(cost)\n",
    "    if i % 100 == 0:\n",
    "        print('Loss after iteration {}: {:.05f}'.format(i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(array_2d, K=5, alpha=0.09, beta=0.001, epochs=25, print_cost=True):\n",
    "    '''This model creates a dynamic features matrix by taking a sparse matrix and learning latent features.\n",
    "    Two matrices are created, randomly initiaized, and trained. Their dot product is similar to the original \n",
    "    sparse matrix except that the latent features also find similarities and add dynamic features to the original matrix.\n",
    "    \n",
    "    Arguments:\n",
    "    array_2d -- 2d array, I call it array because the actual input is a numpy array.\n",
    "    K -- int, dimension of matrix factorization ie. shape(regions, K).\n",
    "    alpha -- float, learning rate for cost function (not the regularization term).\n",
    "    beta -- float, learning rate for regularization term.\n",
    "    epochs -- int, number of epochs for training.\n",
    "    print_cost -- boolean, specifies if costs and plotting of learning rate will be turned on.\n",
    "    \n",
    "    dynamic_features -- 2d array, array with dynamic features.\n",
    "    costs -- if print_cost is true then there will be costs in a list.'''\n",
    "    \n",
    "    # Get shape (time_intervals, regions)\n",
    "    time_intervals = array_2d.shape[0]\n",
    "    regions = array_2d.shape[1]\n",
    "    \n",
    "    # Matrix Factorization\n",
    "    P = np.random.normal(scale=1/K, size=(time_intervals, K))\n",
    "    Q = np.random.normal(scale=1/K, size=(regions, K))\n",
    "\n",
    "\n",
    "    b_time_intervals = np.zeros(time_intervals)\n",
    "    b_regions = np.zeros(regions)\n",
    "    b = np.mean(array_2d[np.where(array_2d!=0)])\n",
    "    \n",
    "    non_zero_examples = non_zero(array_2d, time_intervals)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        np.random.shuffle(non_zero_examples)\n",
    "        \n",
    "        # Update latent matrices. This function will not return anything\n",
    "        # and will perform the updates from gradient descent.\n",
    "        stochastic_grad_descent(non_zero_examples, b, b_time_intervals, b_regions,\n",
    "                               P, Q, alpha, beta)\n",
    "        cost = cost_function(array_2d, b, b_time_intervals, b_regions, P, Q, beta)\n",
    "        \n",
    "        if (i % 100 == 0) & (print_cost == True):\n",
    "            print('Loss after iteration {}: {:.05f}'.format(i, cost))\n",
    "        if (i % 5 == 0) & (print_cost == True):\n",
    "            costs.append(cost)\n",
    "            \n",
    "    dynamic_features = dynamic_features_array(b, b_time_intervals, b_regions, P, Q)\n",
    "      \n",
    "    if print_cost == True:\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.title('learning rate = alpha: {}, beta: {}'.format(alpha, beta))\n",
    "                \n",
    "    return dynamic_features, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model(matrix, K=5, alpha=0.0009, beta=0.001, iterations=2000, print_cost=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4.9962537 ,  3.00121548,  3.13218074,  0.99835592],\n",
       "        [ 4.00222559,  1.9285024 ,  2.87683056,  1.00279667],\n",
       "        [ 0.99798872,  1.00182615,  3.89956505,  4.99696361],\n",
       "        [ 1.0024254 ,  2.08308903,  2.73244663,  4.00128961],\n",
       "        [ 3.06392923,  1.00017569,  4.99895719,  4.00022956]]), [])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29227ef54a8>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDX+Dzd7Jv6ajfm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x292268c1358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
